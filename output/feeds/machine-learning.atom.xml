<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Marifel Barbasa - Machine Learning</title><link href="/" rel="alternate"></link><link href="/feeds/machine-learning.atom.xml" rel="self"></link><id>/</id><updated>2019-02-07T22:15:00-10:00</updated><entry><title>AAAI 2019 Reproducible AI WorkshopÂ Takeaways</title><link href="/reproducible-ai.html" rel="alternate"></link><published>2019-01-30T12:50:00-10:00</published><updated>2019-02-07T22:15:00-10:00</updated><author><name>Marifel Barbasa</name></author><id>tag:None,2019-01-30:/reproducible-ai.html</id><summary type="html">&lt;p&gt;Takeaways from the Reproducible &lt;span class="caps"&gt;AI&lt;/span&gt; workshop that I attended at the &lt;span class="caps"&gt;AAAI&lt;/span&gt; 2019&amp;nbsp;Conference.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;span class="caps"&gt;AI&lt;/span&gt; is a science, and reproducible experiments are key to validating results. However, the field of &lt;span class="caps"&gt;AI&lt;/span&gt; is not known for its reproducibility. I&amp;#8217;ve experienced this firsthand over the past months as a newcomer to the field. For example, while &lt;a href="https://www.kaggle.com/"&gt;Kaggle&lt;/a&gt; is generally agreed upon as a great learning tool for modern machine learning techniques, I find it difficult to learn from their competition kernels. Competitors often fork &lt;a href="https://www.kaggle.com/kernels"&gt;Kaggle kernels&lt;/a&gt; (essentially Jupyter Notebooks or Python/R scripts in the cloud) with mediocrely-structured code and either don&amp;#8217;t change any code or change a few numerical parameters, just to score higher on the leaderboard and publish &amp;#8220;their work&amp;#8221; as a new high-scoring kernel. They propagate not-easily-reproducible code yet benefit from it with community upvotes. Hence, one has to look elsewhere for good coding practices, decent documentation, and reproducibility (such kernels are rare gems on Kaggle). These experiences, along with my background in software engineering and attempting to get used to reading code in Jupyter Notebooks, has led to my interest in the Reproducible &lt;span class="caps"&gt;AI&lt;/span&gt;&amp;nbsp;workshop.&lt;/p&gt;
&lt;p&gt;The full-day Reproducible &lt;span class="caps"&gt;AI&lt;/span&gt; workshop was held at the Association for the Advancement of Artificial Intelligence (&lt;span class="caps"&gt;AAAI&lt;/span&gt;) 2019 Conference on January 27th. The workshop chairs were Yolanda Gil (University of Southern California), Odd Erik Gundersen (Norwegian University of Science and Technology), Satinder Singh (University of Michigan), and Joelle Pineau (McGill University). The purpose of the workshop was to discuss and gather ideas &amp;#8220;to make a roadmap for improving the reproducibility of research result[s] presented at future &lt;span class="caps"&gt;AAAI&lt;/span&gt; conferences and other &lt;span class="caps"&gt;AAAI&lt;/span&gt; publications&amp;#8221; [&lt;a href="https://www.idi.ntnu.no/~odderik/RAI-2019/"&gt;1&lt;/a&gt;]. It is admirable that &lt;span class="caps"&gt;AAAI&lt;/span&gt; and workshop chairs are working on creating strides, no matter how small, towards reproducibility in &lt;span class="caps"&gt;AI&lt;/span&gt; research and focusing on practical solutions rather than attempting to solve the whole problem at once. While actionable results in &lt;span class="caps"&gt;AAAI&lt;/span&gt; conferences and publications were the goal, much of the material presented and ideas discussed are relevant to all &lt;span class="caps"&gt;AI&lt;/span&gt; researchers, engineers, and decision makers. More information about the workshop, including the full list of presentation topics and speakers, can be found &lt;a href="https://www.idi.ntnu.no/~odderik/RAI-2019/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are many contributing factors that make reproducibility within &lt;span class="caps"&gt;AI&lt;/span&gt; research difficult. The popular usage of Jupyter Notebooks within the &lt;span class="caps"&gt;AI&lt;/span&gt; and data science communities might be one of them. Notebooks hardcode their parameters and allow for cells to be run out of order. They don&amp;#8217;t help enforce dependencies, and once you use them, you are pretty much locked into using them. They combine library with experiment code and lump together code and data; overall, that is a recipe for poorly factored code. Notebooks complicate code reviews and make unit testing the code within them difficult. They also do not allow for easy separation of tests from experiments (such as when using assertions in the notebook). Furthermore, it is difficult to build upon code written in notebooks; this is terrible for advancing science. As Joel Grus stated at the workshop, &amp;#8220;&amp;#8216;Notebooks as a source of reproducibility&amp;#8217; presupposes a static view of &lt;span class="caps"&gt;AI&lt;/span&gt; as a science&amp;#8221; [&lt;a href="https://www.idi.ntnu.no/~odderik/RAI-2019/presentations/if_not_notebooks.pdf"&gt;2&lt;/a&gt;]. In my view, the main advantage of notebooks are that they are good for their interactiveness, constructing and consuming conceptual tutorials, and trying something out quickly. But perhaps they should not be used, at least entirely, for &lt;span class="caps"&gt;AI&lt;/span&gt; research&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;Another contributing factor to irreproducible &lt;span class="caps"&gt;AI&lt;/span&gt; research is that machine learning development is much harder than traditional software development. While software is built to meet certain functional specifications, machine learning systems are built to optimize metrics. While software requires quality code, machine learning requires much more than that: quality input data, quality methods, quality metrics, and quality tuning. Production machine learning is even harder since it requires consistent data input; design, retraining, and inference are often carried out by different people; and software must work across different environments. Traditional software development, on the other hand, became faster through development lifecycle tools, and these tools are useful enough that a team of developers can work effectively with one another. It is also not as difficult for a software developer to jump across different teams and projects, due to these tools. Another problem with reproducibility, especially in &lt;span class="caps"&gt;AI&lt;/span&gt;, is that it takes a lot more time and effort to implement. For many, if the benefits of reproducibility are not immediate, then it might not be worth the extra time and effort, since it is the nature of research to publish first or iterate quickly on ideas that might not make it into a&amp;nbsp;paper.&lt;/p&gt;
&lt;p&gt;Now that we&amp;#8217;ve established that reproducibility in &lt;span class="caps"&gt;AI&lt;/span&gt; research is difficult, what can we do to strive towards reproducibility? Proposed solutions are to start with simpler models, then only if behavior is good enough, try to scale up. Proper documentation can go a long way with clearly explained experiments, results, and how to execute the&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;We can assign responsibilities to the following groups of people: paper publishers, reviewers, authors, and the &lt;span class="caps"&gt;AI&lt;/span&gt; community. The main idea is that, if we require authors to do certain things, they will do them if it means that their paper is more likely to get accepted or&amp;nbsp;published.&lt;/p&gt;
&lt;p&gt;We can assign publishers any of the following&amp;nbsp;tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Have a reproducibility track at conferences and establish guidelines around&amp;nbsp;that&lt;/li&gt;
&lt;li&gt;Provide reproducibility guidelines or requirements to paper authors; instead of accomplishing this per publisher, all publishers around the world should agree on a specific set of reproducibility criteria, not for a reproducibility track but for acceptance into publications and&amp;nbsp;conferences&lt;/li&gt;
&lt;li&gt;Create a reward mechanism for original authors whose work has been reproduced by other researchers, such as a Most Reproducible Paper Award or a badge system on published&amp;nbsp;papers&lt;/li&gt;
&lt;li&gt;Find a lightweight way for graduate students to reproduce&amp;nbsp;work&lt;/li&gt;
&lt;li&gt;Require that paper authors have multiple sections to their experiments, such as working with toy problems (if their dataset is too huge for the average researcher to reproduce their experiments and results) and then large-scale&amp;nbsp;problems&lt;/li&gt;
&lt;li&gt;Publishers should provide private data and code repositories for published&amp;nbsp;papers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can assign reviewers any of the following&amp;nbsp;tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Publishers can set a budget and reviewers can then vote on which papers to reproduce; this might help in assessing the reproducibility of certain&amp;nbsp;papers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can assign authors any of the following&amp;nbsp;tasks:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Write an explicit paragraph in the paper itself about reproducibility of the paper&amp;#8217;s experiments and&amp;nbsp;results&lt;/li&gt;
&lt;li&gt;Document everything about the code, data, methods, parameters, and&amp;nbsp;results&lt;/li&gt;
&lt;li&gt;Anonymize data if necessary so that they can share it with the&amp;nbsp;public&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other tasks that the &lt;span class="caps"&gt;AI&lt;/span&gt; community can&amp;nbsp;accomplish:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Some sort of recognition or public system in which publishers or the community can explicitly ask authors whether they think that their work is reproducible; then we can have a web application to crowdsource publisher or community&amp;nbsp;ratings&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In conclusion, in order to achieve reproducibility in &lt;span class="caps"&gt;AI&lt;/span&gt; research, we need to standardize how we measure reproducibility and make extensiblility of &lt;span class="caps"&gt;AI&lt;/span&gt; software systems a focus. The next steps for workshop attendees are to await an email from the workshop chairs to cast our vote on the tasks that should be carried out first. It was suggested that we start small and vote on pressing matters or those that have a higher chance of being carried&amp;nbsp;out.&lt;/p&gt;</content><category term="Machine Learning"></category><category term="ai machine-learning"></category></entry></feed>